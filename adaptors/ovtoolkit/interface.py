#
# Copyright (C) 2020-2023 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0
#

import os
import logging as log
from adaptors.base_adaptor import BaseInterface
from adaptors.ovtoolkit.load_model import ModelLoader
import datetime
import sys
import openvino.runtime as ov

class OvtkInterface(BaseInterface):
    def __init__(self, model_name, path, device):
        super().__init__()
        self.ie = ov.Core()
        self.device = device
        self.net = ''
        self.outputs_len = 0
        self.model_loader = ModelLoader(str(model_name))
        self.model_loader.setModelDir(path)
        self.infer_request = ''
        self.quant_model = False

    def load_model(self, model_xml=None, model_name=None, result={}):
        start_time = datetime.datetime.now()
        if not model_xml:
            log.error("Error !!! model_xml path: \'{}\' missing".format(model_xml))
            result[model_name] = False

        # ---------- 1. Read IR Generated by ModelOptimizer (.xml and .bin files) -------------
        model_bin = os.path.splitext(model_xml)[0] + ".bin"
        log.info("loading network files: \t{} \t{}".format(model_xml, model_bin))
        try:
            self.net = self.ie.read_model(model=model_xml, weights=model_bin)
        except Exception as inst:
            log.warning(inst)
            log.warning("Failure to read model")
            result[model_name] = False
            return

        if(self.quant_model):
            self.device = "CPU"
            log.warning("Forcing device for Quant: "+ self.device)
        log.info("using device: "+ self.device)
        cpu_props = {'PERFORMANCE_HINT': 'LATENCY', "INFERENCE_PRECISION_HINT": "f32"}
        gpu_props = {'PERFORMANCE_HINT': 'LATENCY'}
        exec_net = ''
        try:
            if("GPU" in self.device):
                exec_net = self.ie.compile_model(self.net, self.device, gpu_props)
            else:
                exec_net = self.ie.compile_model(self.net, self.device, cpu_props)
        except Exception as inst:
            log.warning(inst)
            if("CPU" in self.device):
                log.error("Failed to load model")
                result[model_name] = False
                return
            log.warning("using Fallback device CPU ")
            exec_net = self.ie.compile_model(self.net, "CPU", cpu_props)
        self.infer_request = exec_net.create_infer_request()
        self.outputs_len = len(exec_net.outputs)
        curr_time = (datetime.datetime.now() - start_time).total_seconds()
        log.info("Time spent in loading model {}: {}".format(model_name, curr_time))

        result[model_name] = True

    def run_detection(self, input_data):
        start_time = datetime.datetime.now()

        #returns dictionary with keyword as nodename and values :tupple of data and their shape
        response = {}
        processed_data = {}
        if (self.infer_request == '') :
            log.error("Error !!! infer request is null")
            return response
        for key in input_data:
            input_shape = input_data[key][1]
            img = input_data[key][0]
            img = img.reshape(input_shape)
            processed_data[int(key)] = img

        curr_time = datetime.datetime.now()
        res = self.infer_request.infer(inputs=processed_data)
        end_time = datetime.datetime.now()
        predict_time = (end_time - curr_time).total_seconds() * 1000
        for output_key in range(self.outputs_len):
            out = self.infer_request.get_output_tensor(output_key).data
            response[str(output_key)] = (out, list(out.shape))
        exit_time = datetime.datetime.now()
        input_time = (curr_time - start_time).total_seconds() * 1000
        output_time = (exit_time - end_time).total_seconds() * 1000
        log.debug("INPUT_Prep:{} Inference_TIME:{} OUTPUT_Prep:{}".format(input_time,
                                                                      predict_time, output_time))
        return response

    def prepareDir(self):
        self.model_loader.prepareDir()

    def cleanUp(self):
        self.model_loader.cleanUp()

    def saveXML(self, chunk):
        self.model_loader.saveXML(chunk)

    def saveBin(self, chunk):
        self.model_loader.saveBin(chunk)

    def isModelLoaded(self, timeout_in_ms):
        return self.model_loader.isModelLoaded(self, timeout_in_ms)
